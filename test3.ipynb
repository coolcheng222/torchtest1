{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = 'cornell movie-dialogs corpus'\n",
    "corpus = os.path.join('data',corpus_name)\n",
    "def printLines(file,n=10):\n",
    "    with open(file,'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "# printLines(os.path.join(corpus,'movie_lines.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "import torch\n",
    "from torch.jit import script,trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv,random,re,os,unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLines(fileName,fields):\n",
    "    lines = {}\n",
    "    with open(fileName,'r',encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            lineObj = {}\n",
    "            for i,field in enumerate(fields):\n",
    "                lineObj[field] = values[i]\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "    return lines\n",
    "# 传入conversation file,lines,和字段们\n",
    "def loadConversations(fileName,lines,fields):\n",
    "    conversations = []\n",
    "    with open(fileName,'r',encoding=\"iso-8859-1\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            convObj = {}\n",
    "            for i,field in enumerate(fields):\n",
    "                convObj[field] = values[i]\n",
    "            utterance_id_pattern = re.compile('L[0-9]+')\n",
    "            lineIds = utterance_id_pattern.findall(convObj[\"utteranceIDs\"])\n",
    "            convObj[\"lines\"] = []\n",
    "            for lineId in lineIds:\n",
    "                convObj[\"lines\"].append(lines[lineId])\n",
    "            conversations.append(convObj)\n",
    "        return conversations\n",
    "\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation[\"lines\"]) - 1):\n",
    "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            targetLine = conversation[\"lines\"][i + 1][\"text\"].strip()\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine,targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define path to new file\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "\n",
    "# delimiter = '\\t'\n",
    "# # Unescape the delimiter\n",
    "# delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "# # Initialize lines dict, conversations list, and field ids\n",
    "# lines = {}\n",
    "# conversations = []\n",
    "# MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "# MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "# # Load lines and process conversations\n",
    "# print(\"\\nProcessing corpus...\")\n",
    "# lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
    "# print(\"\\nLoading conversations...\")\n",
    "# conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\n",
    "#                                   lines, MOVIE_CONVERSATIONS_FIELDS)\n",
    "\n",
    "# # Write new csv file\n",
    "# print(\"\\nWriting newly formatted file...\")\n",
    "# with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "#     writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "#     for pair in extractSentencePairs(conversations):\n",
    "#         writer.writerow(pair)\n",
    "\n",
    "# # Print a sample of lines\n",
    "# print(\"\\nSample lines from file:\")\n",
    "# printLines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1 # start of sentence\n",
    "EOS_token = 2\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:\"PAD\",SOS_token:\"SOS\",EOS_token:\"EOS\"}\n",
    "        self.num_words = 3\n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD',s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    # print(s)\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # print(s)\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # print(s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\.!\\?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "def readVocs(datafile,corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open(datafile,encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    # 创建一个新的voc和一组对话pair\n",
    "    return voc,pairs\n",
    "def fileterPair(p):\n",
    "    # 根据单词长度滤\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if fileterPair(pair)]\n",
    "\n",
    "# 整合\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Start preparing training data ...\nReading lines...\nRead 221282 sentence pairs\nTrimmed to 64271 sentence pairs\nCounting words...\nCounted words: 18008\n\npairs:\n['there .', 'where ?']\n['you have my word . as a gentleman', 'you re sweet .']\n['hi .', 'looks like things worked out tonight huh ?']\n['you know chastity ?', 'i believe we share an art instructor']\n['have fun tonight ?', 'tons']\n['well no . . .', 'then that s all you had to say .']\n['then that s all you had to say .', 'but']\n['but', 'you always been this selfish ?']\n['do you listen to this crap ?', 'what crap ?']\n['what good stuff ?', 'the real you .']\n"
    }
   ],
   "source": [
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 3\n",
    "def trimRareWords(voc,pairs,MIN_COUNT):\n",
    "    voc.trim(MIN_COUNT)\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_outpus = True\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_outpus = False\n",
    "                break\n",
    "        if keep_input and keep_outpus:\n",
    "            keep_pairs.append(pair)\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "keep_words 7823 / 18005 = 0.4345\nTrimmed from 64271 pairs to 53165, 0.8272 of total\n"
    }
   ],
   "source": [
    "pairs = trimRareWords(voc,pairs,MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc,sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "def zeroPadding(l,fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l,fillvalue=fillvalue))\n",
    "def binaryMatrix(l,value=PAD_token):\n",
    "    m = []\n",
    "    for i,seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputVar(l,voc):\n",
    "    indexes_batch = [indexesFromSentence(voc,sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar,lengths\n",
    "def outputVar(l,voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "# 整合\n",
    "def batch2TrainData(voc,pair_batch):\n",
    "    pair_batch.sort(key = lambda x:len(x[0].split(\" \")),reverse=True)# 按单词数降序\n",
    "    input_batch,output_batch = [],[]\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp,lengths = inputVar(input_batch,voc)\n",
    "    output,mask,max_target_len = outputVar(output_batch,voc)\n",
    "    return inp,lengths,output,mask,max_target_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc,[random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "# print(\"input_variable:\", input_variable)\n",
    "# print(\"lengths:\", lengths)\n",
    "# print(\"target_variable:\", target_variable)\n",
    "# print(\"mask:\", mask)\n",
    "# print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,embedding,n_layers=1,dropout=0):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size,n_layers,dropout = (0 if n_layers == 1 else dropout),bidirectional=True)\n",
    "    def forward(self,input_seq,input_lengths,hidden_size):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # 把padding pack掉,输入三维矩阵和长度\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded,input_lengths)\n",
    "        outputs,hidden = self.gru(packed)\n",
    "        # 只需要处理output,因为output包含时序\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # 将双向相加,内置的处理是dim=2拼接\n",
    "        outputs = outputs[:,:,:self.hidden_size] + outputs[:,:,self.hidden_size:]\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scores](https://pytorch.org/tutorials/_images/scores.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luong\n",
    "class Attn(nn.Module):\n",
    "    # method: dot,general,concat\n",
    "    def __init__(self,method,hidden_size):\n",
    "        super(Attn,self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot','general','concat']:\n",
    "            raise ValueError(self.method,\" is not method!\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            # 内含训练参数W\n",
    "            self.attn = nn.Linear(self.hidden_size,hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            # 内含训练参数W\n",
    "            self.attn = nn.Linear(self.hidden_size * 2,hidden_size)\n",
    "            # 将v加入训练参数\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "    def dot_score(self,hidden,encoder_output):\n",
    "        # 广播乘法再相加,形成内积\n",
    "        return torch.sum(hidden * encoder_output,dim=2)\n",
    "    def general_score(self,hidden,encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy,dim=2)\n",
    "    def concat_score(self,hidden,encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0),-1,-1),encoder_output,2))).tanh()\n",
    "        return torch.sum(self.v * energy,dim=2)\n",
    "    def forward(self,hidden,encoder_output):\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden,encoder_output)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden,encoder_output)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden,encoder_output)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies,dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,attn_model,embedding,hidden_size,output_size,n_layers=1,dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN,self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2,hidden_size)\n",
    "        self.out = nn.Linear(hidden_size,output_size)\n",
    "        self.attn = Attn(attn_model,hidden_size)\n",
    "    def forward(self,input_step,last_hidden,encoder_outputs):\n",
    "        # 先过word embedding和dropout\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # gru过一下\n",
    "        rnn_output,hidden = self.gru(embedded,last_hidden)\n",
    "        # 根据输出和encoder算一下attention\n",
    "        attn_weights = self.attn(rnn_output,encoder_outputs)\n",
    "        # bmm: batch matmul,批量的矩阵乘法,计算一批context矢量\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "        # 一个时间过gru,可以挤压\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        # 拼接后进行Linear再tanh,是luong给出的方案(见计算公式)\n",
    "        concat_input = torch.cat((rnn_output,context),1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # 过一下linear\n",
    "        output = self.out(concat_output)\n",
    "        # 过一下softmax\n",
    "        output = F.softmax(output,dim=1)\n",
    "        return output,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp,target,mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp,1,target.view(-1,1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss,nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 39)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    _,topi = decoderoutput.topk(1)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,embedding,encoder_optimizer,decoder_optimizer,batch_size,clip,max_length=MAX_LENGTH):\n",
    "    # 去掉梯度\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # packing的length参数放cpu\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    # encoder登场\n",
    "    encoder_outputs,encoder_hidden = encoder(input_variable,lengths)\n",
    "\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            # decoder登场\n",
    "            decoder_output,decoder_hidden = decoder(\n",
    "                decoder_input,decoder_hidden,encoder_outputs\n",
    "            )\n",
    "            # 新的input\n",
    "            decoder_input = target_variable[t].view(1,-1)\n",
    "            mask_loss,nTotal = maskNLLLoss(decoder_output,target_variable[t],mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output,decoder_hidden = decoder(\n",
    "                decoder_input,decoder_hidden,encoder_outputs\n",
    "                )\n",
    "            # 取出最大概率的一组\n",
    "            _,topi = decoderoutput.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder.to(device)\n",
    "            mask_loss,nTotal = maskNLLLoss(decoder_output,target_variable[t],mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item * nTotal)\n",
    "            n_totals += nTotal\n",
    "    loss.backward()\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(),clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters,clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bite1fd80d77db0442b89df39c818f3ee81",
   "display_name": "Python 3.7.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}